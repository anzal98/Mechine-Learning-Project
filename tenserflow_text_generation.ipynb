{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONjefr18d2NG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CMLi6mzeV4i",
        "outputId": "458e2ff4-b1b1-4e38-e326-b28c9d3706e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEeUPRkbe0PP",
        "outputId": "2ec8f9e6-8cc2-44bc-b8ad-6f82c78746b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Z9znUiYfa47",
        "outputId": "900f4f11-7bc5-422a-e2c4-b8e318ea798f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUAWPzCkfhVj",
        "outputId": "7c5a69b2-4c0c-424d-be91-f5c813e97879"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJTIm5AUfn-V",
        "outputId": "9b7e80be-73d7-4d05-b015-4008e71be314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "WZGxLMhRfx11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dc8HjCSf4xE",
        "outputId": "35f800ca-1ba9-4f33-de50-e9ba9028c1d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "aD4RZKZff74C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Gf6gBvygAU4",
        "outputId": "3b9ff657-be20-423a-cc0a-b3925b6901cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FltZhDGgHgS",
        "outputId": "df311df2-9730-46dc-fe30-9b09353b0d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "NfRkhQCsgNzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUIy5lBngTh8",
        "outputId": "42113b9a-a205-4a1e-a7ac-0e385f967bc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "diQ6UbvognOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLVY6OOhgqWb",
        "outputId": "773a28c2-12c2-46cf-f273-a1d6dc6dad45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100\n"
      ],
      "metadata": {
        "id": "XrA-W85ogtPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzl-Qi4vgwZG",
        "outputId": "3ae8b765-049c-4a50-ef78-2a16ed3f5a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KARaNhZg0Ps",
        "outputId": "3d0a667b-5c16-48a6-be0e-51815601f353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "uus0Lq-mg3bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFZl7ml5g6aC",
        "outputId": "cdfd783e-73d4-450c-9ebc-c75718fc790c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "Wu8RP60Pg9vS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV04a7D8hBlC",
        "outputId": "e51a6703-4b6e-4f37-91f4-d68baec7e800"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B28PnJwhEQe",
        "outputId": "5cd210e9-a853-4390-b4f5-3d666eeb44dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "Pfuoi9F8hJvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "ZxoI38MWhQHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "L9glfAgqhTKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHn4_agVhWFZ",
        "outputId": "008299b7-85a4-42aa-b20a-acc97b34ccff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR7-aDV0hdTi",
        "outputId": "0855290f-12e0-48c2-c122-12d8d5bf0d71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "MaP3DfQBhhWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "038kDNFdhxbJ",
        "outputId": "ab5c0fc8-981f-47c0-963d-ed033959ca4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([20,  8, 25, 36, 18, 28, 26, 27, 64, 49,  0, 24, 30,  2,  5, 62, 36,\n",
              "        6, 31, 46, 56,  6, 53, 52, 39, 50, 15, 41, 65, 15, 52, 57, 58, 65,\n",
              "       17, 35, 11, 42, 18, 17, 64, 38, 24, 61, 48, 15, 44, 28, 36,  8, 22,\n",
              "       32, 49,  5,  0, 63, 39, 19, 64, 44, 13, 50,  0, 32, 33, 39, 52,  4,\n",
              "       23, 47, 65, 23, 58, 32, 32, 14,  4, 10,  2,  1, 55, 54, 27, 53, 16,\n",
              "       20,  0,  2, 60, 41, 16, 33, 23, 10, 19, 18, 23, 60, 22, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGJFZpN3h0zV",
        "outputId": "e17bbf34-1b26-49e2-d7dd-63583bd45331"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\" and ingenious studies.\\nPisa renown'd for grave citizens\\nGave me my being and my father first,\\nA mer\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"G-LWEOMNyj[UNK]KQ &wW'Rgq'nmZkBbzBmrszDV:cEDyYKviBeOW-ISj&[UNK]xZFye?k[UNK]STZm$JhzJsSSA$3 \\npoNnCG[UNK] ubCTJ3FEJuID\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "dy_cNlWYh5OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjkJ7kEeiAPq",
        "outputId": "5d95f1b8-3c66-4b4e-a693-00cbfab47fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1901283, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzH0ju9CiDiN",
        "outputId": "a65d1b02-76c7-4e7f-cccb-bad110d85302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.031265"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "ulM_WTD5iJLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "sGUgBJ_-iMXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "5TxPCcfDiPyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLFv_oOviUM8",
        "outputId": "1615c2f6-3686-4ec2-b812-d0196463dc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 16s 59ms/step - loss: 2.7062\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.9782\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.6965\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.5370\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.4400\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3744\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.3229\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.2776\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.2377\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 1.1978\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1573\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1151\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0715\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0250\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9763\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9244\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.8713\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 0.8181\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 0.7675\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.7201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "3DsC2PHHiXQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "6xLATpZPjs9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIvo_eNMjwDH",
        "outputId": "1cda0f4f-3c46-4136-b471-afd0bd26dc01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The sly delight they live; to accomplish heavy bless\n",
            "The child on Saint Servingman:\n",
            "I'll have no imprisonment.\n",
            "\n",
            "PETRUCHIO:\n",
            "I hear that Clarence, that he requied to him.\n",
            "\n",
            "BIONDELLO:\n",
            "Then lean, madam, the torce be help!\n",
            "\n",
            "MENENIUS:\n",
            "You'll make me die this day, my lord?\n",
            "\n",
            "BUCKINGHAM:\n",
            "Then 'twas my care is vain to deck.\n",
            "\n",
            "SEBASTIAN:\n",
            "I hope, I\n",
            "make promises for moching.\n",
            "\n",
            "KING EDWARD IV:\n",
            "Taste of submorred uncle; but I am a gentleman oath\n",
            "And in Corioli, and my friends.\n",
            "My horse to what any spare, for he\n",
            "into myself.\n",
            "\n",
            "ANGELO:\n",
            "Nay, it is thus content to be ingrateful.\n",
            "What, ho! Comes here:\n",
            "A sleeve! coldly comes along;\n",
            "Then well here shall crept in each other a grave:\n",
            "Apperhan what is berofful order's kin,\n",
            "And put the nap is Bring a father,\n",
            "Oxfron of my soul and do:\n",
            "Thy old grave cardies, both he wash all\n",
            "To have him against them. I must know\n",
            "The day of noble form-hearted bones:\n",
            "Come I multitide, daughter and her witness:\n",
            "Well, do thy word: she was a man distress you.\n",
            "\n",
            "Calely:\n",
            "and one that's of \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 5.775213718414307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfFFGqfUjyEZ",
        "outputId": "a7ce8d80-5f06-4bf0-fee4-c1f96cf974ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThese ease your son was virtuous to be\\nso entired Valubs?\\n\\nPOMPEY:\\nPray, what's the matter?\\n\\nMost officer:\\nIt stands me? here, will you go your tongue.\\n\\nGREEN:\\nNor I. If is it have been\\nstrucked alive? I am innocern'd in the velume of fire,\\nThat wounds the orator spoken.\\n\\nKING EDWARD IV:\\nYea, a horse being like to be myself, and spure\\nmy mourning in the duke's better; threatening then, at needle\\nwill be begin to star our points; and, for all\\nSo office with his bosom; and where I am not\\nBut if controversy strength and all esseem'd:\\n'Tis far friend the began of a chreak-marr'd-great ancient,\\nNever become more than madam.\\n\\nCAMILLONA:\\nI'll rich him hither grant.\\n\\nFirst Senator:\\nNay, but your patience leave abast.\\n\\nKING EDWARD IV:\\nThanks, noble one, give fear to lose the drum.\\n\\nKING RICHARD II:\\nWell done! fare you woll mercy, when?\\nI am a worthy dile-mark't is too far,\\nAnd so I was her; which, to strike\\nSome case when I may be myself\\nThan was better for her best arrive.\\n\\nHENRY BOLINGBROKE:\"\n",
            " b\"ROMEO:\\nThe one in many tosts, this bearth'd act\\nor here? by whose is me fourteen?\\nThou wast the trial, divined! Never speak, so, for\\nIspair. He is come to do her blemish.\\nIs, throw down I love thee well,\\nThe pay more another state to Jovun\\nAffection that they follow: But who comes here?\\nWeak not this increasith? Father, what a means,\\nTake that to hear I mean to-morrow,\\nTo study as it is, how accursed venom,\\nAnd do were well, and desire your grace were even.\\n\\nHASTINGS:\\nBe join'd with mattle inquired acree!\\nWould he no ill-empting? Show us this?\\nEven I husband what would you have?\\n\\nGLOUCESTER:\\nNo beast! say you hither to a child:\\n'Tis more than fear of yonder,\\nAnd gentlewell account on ore banishment\\nThan what you may say.\\n\\nRATIS:\\n'Twas well for that office of yours you live finger.\\n\\nELBOW:\\nMarry, I thank your goodness so it accountent things;\\nFor we have often handle of a woman's\\nhand, and I will speak me-fresh of all emptison:\\nThose whose duty, I throw doath the rest,\\nFor sleepen, you rogue \"\n",
            " b\"ROMEO:\\nThe old preyares there would pluck him means worse\\nHast your conceit impose,\\nAnd then two worse pardons and Arr's venom'd:\\nYour love can one the but thou return's\\nnor fine that seat it o'er! why\\nIf you do left them at the Doscifit\\nWhile he husband to show business revold\\nUpon their shames by drink alive, and here I find thee darchuned\\nchain,--no boy first and for words,\\nWhich was I big true more our innocence no\\nFrom fortune and your kissing face\\nWhich hides I mean already.\\n\\nBAPTISTA:\\nI'll knock me so, farewell: the deed,\\nThat ever he knew me what I abhor\\nTheir vauntish wreak inchinex and kneel,\\nActide me words and eyes on earth:\\n'Wish further thou dream'st thy siges: that,\\nHe's in Clifer and husbands, and\\nBefore such a loss, but murder not.\\nBerefother, O, that I know, they say.\\n\\nRICHARD:\\n'Tis but to laugh at 'em. Come on, and farewell; and, now, my uncle\\nDismiss when mine eyes can show himself for Romeo;\\nIf ever your fearful king:\\nSo please your honour's reverens, fame and marry hand\"\n",
            " b\"ROMEO:\\nFor if he dared I should know her fortune's king?\\n\\nRUTLAND:\\nWhere give you fear his kingly louds me?\\n\\nBAPTISTAG:\\nI say, a worse, a word with you all on,\\nI will wear by the one fall perish;\\nTo lay us to our proncent and myself\\nBid a more behind; bound in the sacram,\\nThink not of things should die before his life,\\nAnd you were four in debt: he's some another, show.\\nNow, Grumio, if thou carry comest too but\\nbefore the towns about his head and undertake me suffer,\\nAnd hark you, Clarence and despair:\\nThe odds and rock now here, this island's morte,\\nHow many years a bed very day nor Clifform:\\nThis was my for unhappy joy,\\nAnd death, the king that morn their other's rights,\\nThe deed where could have seen 'tis bed,\\nAnd to the beast in speak and aim'd. It will I make\\nYour highness will not tear it.\\n\\nROMEO:\\nIs this the prince can ado, Toward to wime off\\nthe plumes,\\nThat Angelot lengthen'd man,\\nNor made them fired the royal person, of their eyes:\\nHow far brought you here?\\n\\nSICINIUS:\\nHave your roy\"\n",
            " b\"ROMEO:\\nBy love! when I say? A six yield moves?\\n\\nABHORSON:\\nGo to; look to the court.\\n\\nClown:\\nHa! late not, sir.\\n\\nYORK:\\nThe worst is false.\\n\\nESCALUS:\\nThese words is sound of Richmond, is my point of blood,\\nNor chastity well in him a miscreanner:\\nNow I know not what.\\n\\nBIANCA:\\nPardon, madam, no more of that which\\nchance than on his knee.\\n\\nANGELO:\\nYou daughter not ober.\\n\\nFirst Soldier:\\nFollow your fellows, I wot yet we pardon.\\n\\nPOMPEY:\\nI do not ancre-a knock, so your own desire of all myself:\\nYes, I am: it 'suedis of your country's blood,\\nThat seem'd against infinient lusts,\\nThose of his grumbling,--\\n\\nSICINIUS:\\nTo bed, or Clarence, good night.\\n\\nTRANIO:\\n'Tis the diadem.\\n\\nCARIILAN:\\nKindred, fellow: one word, there nothing comes alone:\\nAnd away, mortal; even from his war,\\nSo mother made inquired that the field\\nWhere he should have a husband and affable?\\nWhat, are you mad?\\n\\nBIANCA:\\nIf it be so, for I have it, an you are gentle.\\n\\nHENRY BOLINGBROKE:\\nI hon but my weason seat of bend\\nThat you may rage th\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.121726751327515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGMJurzvj3O0",
        "outputId": "14cf1209-b480-4ff5-c135-b40d4396d573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7b16813c3e80>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcxhCy56j89l",
        "outputId": "77d5ef9c-d012-459e-a7f8-6dbd129744ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "By him, a royal best in death:\n",
            "To put she-privileness, horse from him,\n",
            "And all your ears a maid par\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "q_88i1hYkCR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "r2bk-gK1kGQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "2UUJuvoBkJKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Z775XhkMx8",
        "outputId": "12245439-6c7b-49b0-d0b3-c6eae6b9215e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 17s 54ms/step - loss: 2.7041\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b165dbb61d0>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOEYEak4krel",
        "outputId": "2e883449-3e2f-4647-f620-3c3487779da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1767\n",
            "Epoch 1 Batch 50 Loss 2.0664\n",
            "Epoch 1 Batch 100 Loss 1.9074\n",
            "Epoch 1 Batch 150 Loss 1.8353\n",
            "\n",
            "Epoch 1 Loss: 1.9778\n",
            "Time taken for 1 epoch 13.98 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8053\n",
            "Epoch 2 Batch 50 Loss 1.7020\n",
            "Epoch 2 Batch 100 Loss 1.7003\n",
            "Epoch 2 Batch 150 Loss 1.6489\n",
            "\n",
            "Epoch 2 Loss: 1.7032\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6236\n",
            "Epoch 3 Batch 50 Loss 1.5621\n",
            "Epoch 3 Batch 100 Loss 1.5633\n",
            "Epoch 3 Batch 150 Loss 1.4875\n",
            "\n",
            "Epoch 3 Loss: 1.5454\n",
            "Time taken for 1 epoch 11.51 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4572\n",
            "Epoch 4 Batch 50 Loss 1.5027\n",
            "Epoch 4 Batch 100 Loss 1.4456\n",
            "Epoch 4 Batch 150 Loss 1.4226\n",
            "\n",
            "Epoch 4 Loss: 1.4475\n",
            "Time taken for 1 epoch 11.49 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3458\n",
            "Epoch 5 Batch 50 Loss 1.3513\n",
            "Epoch 5 Batch 100 Loss 1.3881\n",
            "Epoch 5 Batch 150 Loss 1.3585\n",
            "\n",
            "Epoch 5 Loss: 1.3799\n",
            "Time taken for 1 epoch 11.66 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3057\n",
            "Epoch 6 Batch 50 Loss 1.3641\n",
            "Epoch 6 Batch 100 Loss 1.3274\n",
            "Epoch 6 Batch 150 Loss 1.3706\n",
            "\n",
            "Epoch 6 Loss: 1.3280\n",
            "Time taken for 1 epoch 11.26 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2405\n",
            "Epoch 7 Batch 50 Loss 1.3007\n",
            "Epoch 7 Batch 100 Loss 1.2887\n",
            "Epoch 7 Batch 150 Loss 1.3057\n",
            "\n",
            "Epoch 7 Loss: 1.2828\n",
            "Time taken for 1 epoch 11.04 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2664\n",
            "Epoch 8 Batch 50 Loss 1.2384\n",
            "Epoch 8 Batch 100 Loss 1.2955\n",
            "Epoch 8 Batch 150 Loss 1.2579\n",
            "\n",
            "Epoch 8 Loss: 1.2413\n",
            "Time taken for 1 epoch 10.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1399\n",
            "Epoch 9 Batch 50 Loss 1.1979\n",
            "Epoch 9 Batch 100 Loss 1.2121\n",
            "Epoch 9 Batch 150 Loss 1.1637\n",
            "\n",
            "Epoch 9 Loss: 1.2027\n",
            "Time taken for 1 epoch 10.97 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1473\n",
            "Epoch 10 Batch 50 Loss 1.1164\n",
            "Epoch 10 Batch 100 Loss 1.2113\n",
            "Epoch 10 Batch 150 Loss 1.1798\n",
            "\n",
            "Epoch 10 Loss: 1.1629\n",
            "Time taken for 1 epoch 11.12 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ky1ShUKk6RA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}